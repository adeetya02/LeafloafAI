"""
DGAT (Dual Graph Attention Transformer) for Goes Well With Recommendations
SOTA complementary product recommendations using graph neural networks
"""

import numpy as np
import pandas as pd
from typing import Dict, List, Tuple, Optional, Any, Set
from datetime import datetime
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch_geometric.nn import GATConv, global_mean_pool
from torch_geometric.data import Data, DataLoader
import networkx as nx
from sklearn.preprocessing import LabelEncoder
from collections import defaultdict
import joblib
from pathlib import Path


class DualGraphAttentionTransformer(nn.Module):
    """
    DGAT: Distinguishes complementary vs substitute relationships
    Uses dual graph structure with multi-head attention
    """
    
    def __init__(self, n_items: int, n_categories: int, embedding_dim: int = 128,
                 n_heads: int = 8, n_layers: int = 3):
        super().__init__()
        self.n_items = n_items
        self.n_categories = n_categories
        self.embedding_dim = embedding_dim
        
        # Item and category embeddings
        self.item_embeddings = nn.Embedding(n_items, embedding_dim)
        self.category_embeddings = nn.Embedding(n_categories, embedding_dim // 4)
        
        # Dual graph attention layers
        self.complement_layers = nn.ModuleList()
        self.substitute_layers = nn.ModuleList()
        
        for i in range(n_layers):
            in_dim = embedding_dim if i == 0 else embedding_dim * n_heads
            
            # Complement graph layers
            self.complement_layers.append(
                GATConv(in_dim, embedding_dim, heads=n_heads, dropout=0.2)
            )
            
            # Substitute graph layers  
            self.substitute_layers.append(
                GATConv(in_dim, embedding_dim, heads=n_heads, dropout=0.2)
            )
        
        # Cross-graph attention
        self.cross_attention = nn.MultiheadAttention(
            embedding_dim * n_heads, n_heads, dropout=0.2
        )
        
        # Output layers
        self.complement_predictor = nn.Sequential(
            nn.Linear(embedding_dim * n_heads * 2, embedding_dim),
            nn.ReLU(),
            nn.Dropout(0.2),
            nn.Linear(embedding_dim, 1)
        )
        
        self.substitute_predictor = nn.Sequential(
            nn.Linear(embedding_dim * n_heads * 2, embedding_dim),
            nn.ReLU(),
            nn.Dropout(0.2),
            nn.Linear(embedding_dim, 1)
        )
        
        # Initialize weights
        self.apply(self._init_weights)
        
    def _init_weights(self, module):
        if isinstance(module, nn.Linear):
            nn.init.xavier_uniform_(module.weight)
            if module.bias is not None:
                nn.init.zeros_(module.bias)
        elif isinstance(module, nn.Embedding):
            nn.init.normal_(module.weight, mean=0, std=0.02)
    
    def forward(self, item_ids: torch.Tensor, complement_edges: torch.Tensor,
                substitute_edges: torch.Tensor, category_ids: Optional[torch.Tensor] = None):
        """
        Forward pass through dual graph
        Returns: complement scores, substitute scores
        """
        # Get initial embeddings
        x = self.item_embeddings(item_ids)
        
        # Add category information if available
        if category_ids is not None:
            cat_embeds = self.category_embeddings(category_ids)
            x = x + cat_embeds.repeat(1, 4)  # Repeat to match embedding dim
        
        # Process through complement graph
        complement_features = x
        for layer in self.complement_layers:
            complement_features = F.relu(layer(complement_features, complement_edges))
        
        # Process through substitute graph
        substitute_features = x
        for layer in self.substitute_layers:
            substitute_features = F.relu(layer(substitute_features, substitute_edges))
        
        # Cross-graph attention
        comp_attended, _ = self.cross_attention(
            complement_features.unsqueeze(0),
            substitute_features.unsqueeze(0),
            substitute_features.unsqueeze(0)
        )
        comp_attended = comp_attended.squeeze(0)
        
        sub_attended, _ = self.cross_attention(
            substitute_features.unsqueeze(0),
            complement_features.unsqueeze(0),
            complement_features.unsqueeze(0)
        )
        sub_attended = sub_attended.squeeze(0)
        
        # Combine features
        complement_final = torch.cat([complement_features, comp_attended], dim=-1)
        substitute_final = torch.cat([substitute_features, sub_attended], dim=-1)
        
        return complement_final, substitute_final


class GoesWellWithModel:
    """
    Production-ready Goes Well With recommendation model
    Combines DGAT with business rules and seasonal patterns
    """
    
    def __init__(self, config: Dict[str, Any]):
        self.config = config
        self.model_config = config['models']['goes_well_with']
        
        self.model: Optional[DualGraphAttentionTransformer] = None
        self.item_encoder: Optional[LabelEncoder] = None
        self.category_encoder: Optional[LabelEncoder] = None
        
        # Graph structures
        self.complement_graph: Optional[nx.Graph] = None
        self.substitute_graph: Optional[nx.Graph] = None
        self.co_purchase_matrix: Optional[np.ndarray] = None
        
        # Business rules
        self.cuisine_rules = self._load_cuisine_rules()
        self.meal_rules = self._load_meal_rules()
        self.nutrition_rules = self._load_nutrition_rules()
        
        # Caches
        self.complement_cache: Dict[str, List[Tuple[str, float]]] = {}
        
    def _load_cuisine_rules(self) -> Dict[str, List[str]]:
        """Load cuisine-specific pairing rules"""
        return {
            "italian": {
                "pasta": ["tomato_sauce", "parmesan", "basil", "olive_oil", "garlic"],
                "tomato": ["mozzarella", "basil", "olive_oil", "pasta"],
                "pizza_dough": ["tomato_sauce", "mozzarella", "pepperoni", "mushrooms"]
            },
            "mexican": {
                "tortilla": ["beans", "cheese", "salsa", "avocado", "lime"],
                "avocado": ["lime", "cilantro", "tomato", "onion"],
                "beans": ["rice", "cheese", "tortilla", "cilantro"]
            },
            "asian": {
                "rice": ["soy_sauce", "vegetables", "sesame_oil", "ginger"],
                "noodles": ["soy_sauce", "vegetables", "sesame_oil", "garlic"],
                "tofu": ["soy_sauce", "ginger", "scallions", "sesame"]
            },
            "american": {
                "burger": ["bun", "cheese", "lettuce", "tomato", "pickles"],
                "hot_dog": ["bun", "mustard", "ketchup", "relish", "onions"],
                "chicken": ["bbq_sauce", "coleslaw", "corn", "potatoes"]
            }
        }
    
    def _load_meal_rules(self) -> Dict[str, Dict[str, List[str]]]:
        """Load meal-based pairing rules"""
        return {
            "breakfast": {
                "eggs": ["bacon", "toast", "cheese", "milk"],
                "cereal": ["milk", "banana", "berries"],
                "pancakes": ["syrup", "butter", "berries", "bacon"]
            },
            "lunch": {
                "sandwich_meat": ["bread", "cheese", "lettuce", "tomato"],
                "soup": ["crackers", "bread", "salad"],
                "salad": ["dressing", "croutons", "cheese"]
            },
            "dinner": {
                "steak": ["potatoes", "vegetables", "wine", "salad"],
                "fish": ["lemon", "rice", "vegetables", "white_wine"],
                "pasta": ["sauce", "cheese", "bread", "salad"]
            }
        }
    
    def _load_nutrition_rules(self) -> Dict[str, List[str]]:
        """Load nutrition-based pairing rules"""
        return {
            "protein_complements": {
                "beans": ["rice"],  # Complete protein
                "peanut_butter": ["whole_wheat_bread"],
                "hummus": ["pita"]
            },
            "vitamin_absorption": {
                "spinach": ["citrus", "tomatoes"],  # Iron + Vitamin C
                "carrots": ["olive_oil"],  # Beta carotene + Fat
                "tomatoes": ["olive_oil"]  # Lycopene + Fat
            }
        }
    
    def build_product_graphs(self, interactions_df: pd.DataFrame) -> Tuple[nx.Graph, nx.Graph]:
        """Build complement and substitute graphs from interaction data"""
        # Create co-purchase matrix
        sessions = interactions_df.groupby('session_id')['product_id'].apply(list).tolist()
        
        # Count co-occurrences
        co_purchase_counts = defaultdict(int)
        product_counts = defaultdict(int)
        
        for session in sessions:
            for i, prod1 in enumerate(session):
                product_counts[prod1] += 1
                for j, prod2 in enumerate(session):
                    if i != j:
                        co_purchase_counts[(prod1, prod2)] += 1
        
        # Create complement graph (high co-purchase)
        complement_graph = nx.Graph()
        substitute_graph = nx.Graph()
        
        # Add all products as nodes
        all_products = list(product_counts.keys())
        complement_graph.add_nodes_from(all_products)
        substitute_graph.add_nodes_from(all_products)
        
        # Calculate PMI (Pointwise Mutual Information) for edges
        total_sessions = len(sessions)
        
        for (prod1, prod2), count in co_purchase_counts.items():
            # PMI = log(P(x,y) / (P(x) * P(y)))
            p_xy = count / total_sessions
            p_x = product_counts[prod1] / total_sessions
            p_y = product_counts[prod2] / total_sessions
            
            pmi = np.log(p_xy / (p_x * p_y + 1e-8))
            
            # High PMI = complementary (bought together)
            if pmi > 0.5:
                complement_graph.add_edge(prod1, prod2, weight=pmi)
            # Negative PMI might indicate substitutes (rarely bought together)
            elif pmi < -0.5 and self._same_category(prod1, prod2):
                substitute_graph.add_edge(prod1, prod2, weight=abs(pmi))
        
        self.complement_graph = complement_graph
        self.substitute_graph = substitute_graph
        
        return complement_graph, substitute_graph
    
    def _same_category(self, prod1: str, prod2: str) -> bool:
        """Check if two products are in the same category"""
        # Simplified - in practice use actual category data
        return any(keyword in prod1.lower() and keyword in prod2.lower() 
                  for keyword in ['milk', 'bread', 'cheese', 'yogurt'])
    
    def prepare_training_data(self, interactions_df: pd.DataFrame) -> Dict[str, Any]:
        """Prepare graph data for training"""
        # Build graphs
        comp_graph, sub_graph = self.build_product_graphs(interactions_df)
        
        # Encode items
        all_products = list(set(interactions_df['product_id'].unique()))
        self.item_encoder = LabelEncoder()
        self.item_encoder.fit(all_products)
        
        # Extract categories (simplified)
        categories = []
        for product in all_products:
            # Extract category from product name or use metadata
            if 'organic' in product.lower():
                categories.append('organic')
            elif any(dairy in product.lower() for dairy in ['milk', 'cheese', 'yogurt']):
                categories.append('dairy')
            elif any(meat in product.lower() for meat in ['chicken', 'beef', 'pork']):
                categories.append('meat')
            else:
                categories.append('general')
        
        self.category_encoder = LabelEncoder()
        self.category_encoder.fit(categories)
        
        # Convert graphs to edge lists
        comp_edges = []
        for u, v in comp_graph.edges():
            if u in self.item_encoder.classes_ and v in self.item_encoder.classes_:
                u_idx = self.item_encoder.transform([u])[0]
                v_idx = self.item_encoder.transform([v])[0]
                comp_edges.append([u_idx, v_idx])
                comp_edges.append([v_idx, u_idx])  # Undirected
        
        sub_edges = []
        for u, v in sub_graph.edges():
            if u in self.item_encoder.classes_ and v in self.item_encoder.classes_:
                u_idx = self.item_encoder.transform([u])[0]
                v_idx = self.item_encoder.transform([v])[0]
                sub_edges.append([u_idx, v_idx])
                sub_edges.append([v_idx, u_idx])
        
        return {
            'n_items': len(self.item_encoder.classes_),
            'n_categories': len(self.category_encoder.classes_),
            'complement_edges': torch.LongTensor(comp_edges).t() if comp_edges else torch.LongTensor([[0], [0]]),
            'substitute_edges': torch.LongTensor(sub_edges).t() if sub_edges else torch.LongTensor([[0], [0]]),
            'item_categories': self.category_encoder.transform(categories)
        }
    
    def train(self, interactions_df: pd.DataFrame, epochs: int = 30) -> Dict[str, float]:
        """Train the DGAT model"""
        print("Training Goes Well With Model (DGAT)...")
        
        # Prepare data
        data = self.prepare_training_data(interactions_df)
        
        # Initialize model
        self.model = DualGraphAttentionTransformer(
            n_items=data['n_items'],
            n_categories=data['n_categories'],
            embedding_dim=self.model_config.get('embedding_dim', 128),
            n_heads=self.model_config.get('n_heads', 8),
            n_layers=self.model_config.get('n_layers', 3)
        )
        
        # For simplified training, we'll use the graph structure directly
        # In practice, you'd create positive/negative samples and train with BCE loss
        
        optimizer = torch.optim.Adam(self.model.parameters(), lr=0.001)
        
        # Simplified training loop
        item_ids = torch.arange(data['n_items'])
        category_ids = torch.LongTensor(data['item_categories'])
        
        losses = []
        for epoch in range(epochs):
            self.model.train()
            
            # Forward pass
            comp_features, sub_features = self.model(
                item_ids,
                data['complement_edges'],
                data['substitute_edges'],
                category_ids
            )
            
            # Simplified loss - in practice use proper graph loss
            loss = torch.mean(comp_features.pow(2)) + torch.mean(sub_features.pow(2))
            
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()
            
            losses.append(loss.item())
            
            if (epoch + 1) % 10 == 0:
                print(f"Epoch {epoch+1}/{epochs}, Loss: {loss.item():.4f}")
        
        # Cache some common complements
        self._build_complement_cache()
        
        return {
            "final_loss": losses[-1],
            "n_complement_edges": len(data['complement_edges'][0]) // 2,
            "n_substitute_edges": len(data['substitute_edges'][0]) // 2
        }
    
    def _build_complement_cache(self):
        """Pre-compute common complement pairs"""
        if self.complement_graph is None:
            return
        
        # Cache top complements for each product
        for node in self.complement_graph.nodes():
            neighbors = list(self.complement_graph.neighbors(node))
            if neighbors:
                # Sort by edge weight
                weighted_neighbors = [
                    (n, self.complement_graph[node][n].get('weight', 1.0))
                    for n in neighbors
                ]
                weighted_neighbors.sort(key=lambda x: x[1], reverse=True)
                self.complement_cache[node] = weighted_neighbors[:10]
    
    def get_recommendations(self, product_ids: List[str], n_recommendations: int = 5,
                          context: Optional[Dict[str, Any]] = None) -> List[Dict[str, Any]]:
        """Get complementary product recommendations"""
        recommendations = []
        seen_products = set(product_ids)
        
        # Get graph-based recommendations
        graph_recs = self._get_graph_recommendations(product_ids, seen_products)
        
        # Get rule-based recommendations
        rule_recs = self._get_rule_recommendations(product_ids, context)
        
        # Combine and score
        all_recs = {}
        
        # Add graph recommendations
        for rec in graph_recs:
            if rec['product_id'] not in all_recs:
                all_recs[rec['product_id']] = rec
            else:
                all_recs[rec['product_id']]['score'] += rec['score']
        
        # Add rule recommendations
        for rec in rule_recs:
            if rec['product_id'] not in all_recs:
                all_recs[rec['product_id']] = rec
            else:
                all_recs[rec['product_id']]['score'] += rec['score']
                all_recs[rec['product_id']]['reason'] += " + " + rec['reason']
        
        # Sort and filter
        final_recs = list(all_recs.values())
        final_recs.sort(key=lambda x: x['score'], reverse=True)
        
        # Ensure diversity
        diverse_recs = []
        categories_seen = set()
        
        for rec in final_recs:
            # Simple category extraction
            category = 'general'
            for cat in ['dairy', 'meat', 'produce', 'bakery']:
                if cat in rec['product_id'].lower():
                    category = cat
                    break
            
            if category not in categories_seen or len(categories_seen) < 3:
                diverse_recs.append(rec)
                categories_seen.add(category)
                
                if len(diverse_recs) >= n_recommendations:
                    break
        
        return diverse_recs
    
    def _get_graph_recommendations(self, product_ids: List[str], 
                                 seen_products: Set[str]) -> List[Dict[str, Any]]:
        """Get recommendations from graph structure"""
        recommendations = []
        
        for product_id in product_ids:
            # Check cache first
            if product_id in self.complement_cache:
                for complement, weight in self.complement_cache[product_id]:
                    if complement not in seen_products:
                        recommendations.append({
                            "product_id": complement,
                            "score": float(weight),
                            "recommendation_type": "goes_well_with",
                            "reason": f"Frequently bought with {product_id}",
                            "relationship": "complement"
                        })
        
        return recommendations
    
    def _get_rule_recommendations(self, product_ids: List[str], 
                                context: Optional[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """Get rule-based recommendations"""
        recommendations = []
        
        for product_id in product_ids:
            product_lower = product_id.lower()
            
            # Check cuisine rules
            for cuisine, rules in self.cuisine_rules.items():
                for base, complements in rules.items():
                    if base in product_lower:
                        for comp in complements:
                            recommendations.append({
                                "product_id": comp,
                                "score": 0.8,
                                "recommendation_type": "goes_well_with",
                                "reason": f"{cuisine.capitalize()} cuisine pairing",
                                "relationship": "cuisine_rule"
                            })
            
            # Check meal rules
            if context and 'meal_type' in context:
                meal_type = context['meal_type']
                if meal_type in self.meal_rules:
                    for base, complements in self.meal_rules[meal_type].items():
                        if base in product_lower:
                            for comp in complements:
                                recommendations.append({
                                    "product_id": comp,
                                    "score": 0.7,
                                    "recommendation_type": "goes_well_with",
                                    "reason": f"Perfect for {meal_type}",
                                    "relationship": "meal_rule"
                                })
        
        return recommendations
    
    def save(self, model_path: Path) -> None:
        """Save model and metadata"""
        model_path.mkdir(parents=True, exist_ok=True)
        
        # Save PyTorch model
        if self.model is not None:
            torch.save(self.model.state_dict(), model_path / "model.pt")
        
        # Save graphs
        if self.complement_graph is not None:
            nx.write_gpickle(self.complement_graph, model_path / "complement_graph.gpickle")
        if self.substitute_graph is not None:
            nx.write_gpickle(self.substitute_graph, model_path / "substitute_graph.gpickle")
        
        # Save metadata
        joblib.dump({
            'item_encoder': self.item_encoder,
            'category_encoder': self.category_encoder,
            'complement_cache': self.complement_cache,
            'cuisine_rules': self.cuisine_rules,
            'meal_rules': self.meal_rules,
            'nutrition_rules': self.nutrition_rules,
            'config': self.model_config
        }, model_path / "metadata.pkl")
        
        print(f"Goes Well With model saved to {model_path}")
    
    def load(self, model_path: Path) -> None:
        """Load model and metadata"""
        # Load metadata
        metadata = joblib.load(model_path / "metadata.pkl")
        self.item_encoder = metadata['item_encoder']
        self.category_encoder = metadata['category_encoder']
        self.complement_cache = metadata['complement_cache']
        self.cuisine_rules = metadata['cuisine_rules']
        self.meal_rules = metadata['meal_rules']
        self.nutrition_rules = metadata['nutrition_rules']
        
        # Load graphs
        comp_graph_path = model_path / "complement_graph.gpickle"
        if comp_graph_path.exists():
            self.complement_graph = nx.read_gpickle(comp_graph_path)
        
        sub_graph_path = model_path / "substitute_graph.gpickle"
        if sub_graph_path.exists():
            self.substitute_graph = nx.read_gpickle(sub_graph_path)
        
        # Load model
        model_path_file = model_path / "model.pt"
        if model_path_file.exists():
            n_items = len(self.item_encoder.classes_)
            n_categories = len(self.category_encoder.classes_)
            
            self.model = DualGraphAttentionTransformer(
                n_items=n_items,
                n_categories=n_categories,
                embedding_dim=self.model_config.get('embedding_dim', 128),
                n_heads=self.model_config.get('n_heads', 8),
                n_layers=self.model_config.get('n_layers', 3)
            )
            self.model.load_state_dict(torch.load(model_path_file))
            self.model.eval()
        
        print(f"Goes Well With model loaded from {model_path}")